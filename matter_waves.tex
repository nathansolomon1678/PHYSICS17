\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz-cd}

\begin{document}

\title{Matter Waves}
\author{Nathan Solomon}
\maketitle

\section{Cool Integral Trick}
Before starting the actual notes, here's a fun integral. It's particularly useful for solving all those problems where you average something over a probability distribution. It assumes $\operatorname{Re}(a)>0$ and $n \in \mathbb{N}$.
\begin{align*}
    \int_0^\infty x^n e^{-x/a} dx &= \int_0^\infty (a^n y^n) e^{-y} (a \, dy) \\
    &= a^{n+1} \int_0^\infty y^n e^{-y} dy \\
    &= a^{n+1} \Gamma(n+1) \\
    &= n! \, a^{n+1}
\end{align*}

\section{Another Unrelated Topic}
We should memorize this definition of the fine-structure constant:
\[ \alpha := \frac{e^2}{4 \pi \varepsilon_0 \hbar c} = \frac{e^2}{2 \varepsilon_0 h c} \approx \frac{1}{137} \]
It was also recommended that we remember the following approximations:
\[ \hbar c \approx 197 \text{ eV nm} \]
\[ \frac{e^2}{4 \pi \varepsilon_0} \approx 1.44 \text{ eV nm} \]

\section{Dispersion Relations}
This is pretty much all we need to know about dispersion relations (for now):
\[ v_\text{phase} = \omega / k \]
\[ v_\text{group} = \frac{\partial \omega}{\partial k} \]
For all matter waves, the geometric mean of the phase velocity and the group velocity is $c$, the speed of light. For non-dispersive waves, such as light traveling through a vacuum, the phase and group velocities are both $c$.

\section{Uncertainty Principle}
Suppose you have some signal, which is a function of time, and you measure $n$ samples from it, which are evenly spaced time $\Delta t$ apart. Now you know that that signal exists within a window of time with width $\sigma_t := n \Delta t$, so that represents your ``uncertainty in time". But suppose you also want to measure the frequency of the signal. According to the Nyquist-Shannon sampling theorem, the highest frequency you can extract from your samples is $1/(2 \Delta t)$. Trying to measure the frequencies that exist in the signal is equivalent to trying to reconstruct the signal using the sum of several sinusoids, and in this case, you need $n$ sinusoids whose frequencies are evenly spaced, ranging from zero to $1/(2\Delta t)$ (including the upper bound, but not including zero). That means the lowest frequency you can measure using the $n$ samples is $\sigma_f := 1/(2n\Delta t)$, which represents your ``uncertainty in frequency". This allows you to write the uncertainty principle, which is true no matter what you choose $n$ and $\Delta t$ to be.
\[ \sigma_t \sigma_f \geq \frac{1}{2} \]
However, if you define $\sigma$ to be the standard deviation of a function, instead of the width of the window where the function is non-zero, then you can get an even smaller bound. In next week's notes, we'll see how to minimize that bound and do a whole bunch of other cool stuff with that.

\section{Fourier Inversion Theorem}
The inverse Fourier transform of the Fourier transform of $f(x)$ is
\begin{align*}
    \mathcal{F}^{-1}(\mathcal{F}(f))(x) &= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \left( \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty f(x') e^{-ikx} dx' \right) e^{ikx} dk \\
    &= \int_{-\infty}^\infty f(x') \left( \frac{1}{2 \pi} \int_{-\infty}^\infty e^{-ik(x - x')} dk \right) dx' \\
    &= \int_{-\infty}^\infty f(x') \delta(x - x') dx' \\
    &= f(x)
\end{align*}
This proves that $\mathcal{F}^{-1}$ and $\mathcal{F}$, as we have defined them, are indeed inverses, which is good. That proof above uses the Dirac $\delta$ ``function", which is more like the limit of a function than an actual function, but it makes everything work out nicely, so we leave out the nitty-gritty details.

\end{document}
